---
layout: post
title: 使用sbt编译打包scala程序
categories: 
 - BigData
permalink: /posts/3
nocomments: true  # Disable the comment box. This is EasyBook feature
---

Spark应用程序支持采用Scala、python、Java、R等语言进行开发。在Spark Shell中进行交互编程时，可以采用Scala和Python语言，主要是方便对代码进行调试，但需要助航代码的方式运行。一般等到代码都调试好之后，可以选择将Spark打包成为独立的应用程序，然后提交到Spark中运行。如果不是在Spark shell中交互编程，比如使用Java或Scala语言对spark进行编译，也需要编译打包后再提交给Spark运行。采用scala编写的程序，需要使用sbt进行编译打包；采用Java编写的程序，建议使用maven进行编译打包；采用python编写的程序，可以直接通过spark-submit提交给spark运行。今天首先介绍sbt编译打包scala程序。

### 准备工作 ###

首先，安装Java1.7及以上版本，并配置好环境变量，本机配置的java环境是1.8.0_161-b12。其次，安装Hadoop并配置相关环境，spark会用到HDFS和yarn。

### 安装scala并进行配置 ###

这里介绍macOS环境下scala的安装(其他环境大同小异)。这里建议安装2.9.3以上的版本，最好安装2.11中的版本。我之前在配置的时候安装的是2.13中的版本，但是在后面打包sbt的时候会出现因为版本问题而打包不成功的问题，所以我建议安装2.11中的scala。点击链接进入https://www.scala-lang.org/download/

在官网中进行下载是其中一种下载办法，作为程序开发人员，更推荐的办法是在命令行中使用`brew install scala` 的方法，他会直接帮助你下载到/usr/local目录下面。（但是最近mac系统大部分人会更新到macOS calalina版本，这个版本目前还不支持homebrew的方法，还是建议在官网手动下载）

手动下载好的scala的地址应该是在`/Users/haoyingkai/Downloads`下面，使用命令` sudo tar -zxf /Users/haoyingkai/downloads/scala-2.11.1.tgz -C /usr/local/`将下载好的scala解压到`/usr/local`下面，然后进入到/usr/local目录下面，使用`sudo mv ./scala-2.11.1 ./scala`将文件夹名字改一下方便之后配置。

scala安装成功后，`vim /etc/profile`在其中配置scala环境变量如下：

```
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
```

保存之后`source /etc/profile`使配置生效，然后运行`scala`，若出现scala版本和java版本即证明安装成功。

### 安装spark并进行配置 

在官方网站http://spark.apache.org/downloads.html

下载2.4.4版本（可以根据官网提示内容下载）。同scala一样，路径在`/Users/haoyingkai/downloads`下面，要将它解压到`/usr/local`下面。命令同上。同时也将spark-2.4.4-bin-hadoop2.7改为spark。

首先`vim /etc/profile`配置spark环境变量

```
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
```

`source /etc/profile`使之生效。

其次，我们进入`/usr/local/spark/conf`目录下面，执行拷贝命令：`cp spark-env.sh.template spark-env.sh`，并打开该文件`vim spark-env.sh`向其中添加如下内容：

```
export SCALA_HOME=/usr/local/scala

export SPARK_MASTER_IP=localhost

export SPARK_WORKER_MEMORY=4g
```

接下来我们运行`spark-shell`就可以使用spark。

### 使用sbt编译打包scala ###

在https://www.scala-sbt.org/download.html

下载sbt-1.3.2.tgz,解压后在它的bin目录中找到sbt-launch.jar文件，并将其复制到`/usr/local/sbt/`（这个目录需要提前创建）下面，然后，需要编辑shell脚本文件`vim /usr/local/sbt/sbt`，用于启动sbt:

```
#!/bin/bash
SBT_OPTS="-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M"
java $SBT_OPTS -jar `dirname $0`/sbt-launch.jar "$@" 
```

保存之后，使用`chmod u+x /usr/local/sbt/sbt`为该shell脚本添加课执行权限。

然后，我们处理spark，如下：

创建程序根目录：

`mkdir /usr/local/sparkapp`

创建程序所需的文件夹结构：

`mkdir -p /usr/local/sparkapp/src/main/scala`

创建一个SimpleApp.scala文件`vim /usr/local/sparkapp/src/main/scala/SimpleApp.scala`,具体内容如下：

```
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp
{
    def main(args: Array[String])
    {
        val logFile = "file:///usr/local/spark/README.md"
        val conf = new SparkConf().setAppName("Simple Application")
        val sc = new SparkContext(conf)
        val logData = sc.textFile(logFile, 2).cache()
        val numAs = logData.filter(line => line.contains("a")).count()
        val numBs = logData.filter(line => line.contains("b")).count()
        println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
    }
}
```

创建simple.sbt文件`vim /usr/local/sparkapp/simple.sbt`,具体内容如下：

```
name := "Simple Project"
version := "1.0"
scalaVersion := "2.11.10"
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.4.4"
```

这里的scala和spark的版本各自不同，大家自己查看。

最后，在`/usr/local/sparkapp/`下面执行打包操作：

`/usr/local/sbt/sbt package`

这里如果是第一次打包的话，会花很长时间下载相关文件，大家耐心等待。如果有些文件下载不成功的话，大家仔细查看下载文件中scala和spark的版本问题，然后对simple.sbt文件进行更改，再尝试。

成功的话，会显示success和打包成功时间。

package完成之后，我们就会生成相关的jar包，利用jar包，通过spark-submit提交到spark中运行，命令如下：

` /usr/local/spark/bin/spark-submit --class "SimpleApp" /usr/local/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar`

程序成功结果如下：

`Line with a:62, Line with b: 31`

### 总结

这个编译打包的过程中会出现很多的问题，大多都是和版本问题有关，大家要仔细查看报错信息寻找问题。在最后一步中如果运行不成功可以在`/usr/local/spark/jars`中查看spark-core的版本号，再改simple.sbt。

总之，看到小问题大家不要气馁，出去吃个饭回来再看说不定就有思路了。祝大家运行成功！！！

